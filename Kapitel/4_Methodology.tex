\chapter{Methodology and Implementation}
\label{cha:Chapter4_Methodology}

Length: 20-25 pages

Effort: 8 weeks+

Teilung zwischen Methodik (eher abstrakt) und Implementierung
Damit beginnen --> also mit Implementierung

Questions:
\begin{itemize}
\item Structure --> differentiation between getting the data, analysing the data and evaluating the results ok? --> Kann man machen
\item General idea: ``Readers should be able to carry out the same procedure using the thesis'' --> level of detail, e.g. cloud services? --> Implementierung auch auf Github zur Verfuegung stellen, API auf hoeherem Level (was machen die Funktionen verwenden),
Falls Cloud: nur virtueller Rechner --> eher nicht wichtig, falls spezifische Dienstleistung z.B. OpenAI (KI, Parsing) --> dann detaillierter, eher Richtung Implementation
\item Case study --> Evaluation of accuracy based on one real life topic? Topic, e.g. Covid-19 pandemic, german elections (aufpasssen, Politik schwer!), ...? --> je nach Umfang auch mehrere (sollen sich unterscheiden, Covid sehr viele Bereiche), Tweets auch auf Englisch
\end{itemize}

Content
\begin{itemize}
\item Data 
\begin{itemize}
    \item What data does twitter provide, auch z.B. Likes, Retweets
    \item Data extraction, Cloud Services --> spezifischer Dienst?
\end{itemize}
\item Analysis
\begin{itemize}
    \item Lexicon-Based Method
    \item Machine Learning Based Method
    \item Hybrid Method
\end{itemize}
\item Evaluation
\begin{itemize}
    \item Parameters --> objektive Kriterien, Qualitaet der Analyse Methode + Begruendung der Kriterien, Komplexitaet (Laufzeitkomplexitaet ja, Implementierung --> an sich nicht schlimm, aber robust? Abstuerze?)
    \item Case Study
    \item Qualitaet der Ergebnisse --> gut/schlecht, kann das akkurat erkannt werden?, evtl. menschliche Analyse, schoener: auf subjektive Einschaetzung verzichten
    \item Comparison with human tweets labeled by humans?
\end{itemize}
\end{itemize}


Analysis
\begin{itemize}
    \item Lexicon-Based Method
    \begin{itemize}
    \item General approach: Multiple lexicons with different topics, positive/neutral/negative
    \begin{itemize}
        \item Sentiment Words --> List of words with polarity, e.g. good
        \item Negation Words --> Words that negate the polarity of other words, e.g. not
        \item Intensity words --> Words that amplify the polarity, e.g. very
        \item Emoticons
    \end{itemize}
    \item Find the above word types in tweet
    \item Adjust polarity of sentiment words based on negation and/or intensity
    \item Calculate average of polarities including emoticons
    \item Example: The weather today is very terrible but the sound of rain is delightful.
    \begin{itemize}
        \item Sentiment of "very terrible": amplified negative (e.g. -2)
        \item Sentiment of "delightful": positive (e.g. +1)
        \item Overall: -2 + 1 = -1 --> negative
    \end{itemize}
    \item Comparison of different sentiment lexicons?
    \item Stop words --> needs, wants, etc. --> automatically negative (except for negation?)
    \item common idioms?
    \item slang
    \end{itemize}
    \item Machine Learning Based Method
    \begin{itemize}
        \item Naive Bayes --> probabilistic, multiple classes
        \begin{itemize}
            \item Based on Bayes theorem with conditional independence, add-one (laplace) smoothing, calculate P(x|y)
            \item Look at every word contained in tweet, apply formula to known words:
\[P(\mathrm{w}_{i}^{}|c) = \frac{count(\mathrm{w}_{i}^{}, c)+1}{(\sum_{w\in V}^{} (count(w,c)) + \left| V \right|} \]
            \item w = word, c is class (negative, positive), V is vocabulary of all known words
            \item Train with labelled tweets
           \[ \mathrm{c}_{NB}^{} = argmax_{c \in C}  P(c) \prod_{i \in positions}^{} P(\mathrm{w}_{i}^{}|c)\]
          
           
        \end{itemize}
        \item Logistic Regression? --> discriminative, binary classes
        \begin{itemize}
        \item Advantage: Not as many assumptions --> works better even when some features are correlated
        \end{itemize} 
        
    \end{itemize}
    \item Hybrid Method
    \begin{itemize}
        \item Approach 1 --> use lexicon based score as additional feature for classifier (even if 0, it still can be classified)
        \item Combine previous two or e.g. use a different classifier if that works better?
    \end{itemize}
\end{itemize}
\

New Learnings
\begin{itemize}
    \item Evaluation of different classifiers --> (J48), Naive Bayes (gaussian + multinominal distribution), Support Vector Machine, Logistic Regression, Random Forest
    \item Currently best: Naive Bayes with a multinomial distribution, 78.67\% of 4726 instances correct
    \item Caveats: Training for SVM and Logistic Regression not possible with all 1.2MM tweets --> system memory/runtime
    \item TODO: currently the ML method only recognized positive and neutral, more test instances?
    \item Lexicon Method: 75.68\% accurate on 9057 instances (includes neutral)
\end{itemize}

RandomForest
\begin{itemize}
    \item Collection of tree-structured classifiers
    \item Each tree depends on values of a random, independent vector
    \item Output is class selected by most trees
\end{itemize}

Questions
\begin{itemize}
    \item Fundamentals vs. Methodology: Wo z.B. Classifiers erklären?
    \item Welche Classifier wie vertiefen?
    \begin{itemize}
    \item Tabelle die Classifier vergleicht
    \item Kurze Erklärung aller Classifier?
    \item "Besten" Classifier tiefer erklären?
    \item z.B. Naive Bayes: Multinominale und Gaußsche Verteilung vergleichen/erklären?
    \end{itemize}
    \item Spezifische Bibliothek erwähnen (weka), Filter genauer (speichert nur 15000 Wörter, die mindestens 10x vorkommen, lowercase)?
    \item Daten beschreiben: Referenz + kurzer Überblick (Anzahl positiv/neutral/negativ, Themen, Zeitraum, bei Testdaten die Methode zur Klassifikation)
\end{itemize}

\section{Lexicon Method}
As previously mentioned, the lexicon method uses several dictionaries in order to classify a tweet. 
TODO preprocess, negation thing
\begin{algorithm}[H]
  \caption{Lexicon algorithm}\label{euclid}
\begin{algorithmic}[1]
    \Procedure{analyze}{$tweet$}\Comment{Sentiment score of tweet}
\State $SentimentLexicon \gets$ Dictionary containing sentiment words with their polarities
\State $NegationList \gets$ List containing negation words
\State $IntensityLexicon \gets$ Dictionary containing intensity words with their multipliers
\State $EmojiLexicon \gets$ Dictionary containing UTF-8 emojis with their polarities
\ForEach {$word \in tweet$}
\State $score \gets 0.0$
\If{$word \in SentimentLexicon$} 
    \State $polarity \gets 1$
    \For{\texttt{previous two words}}
        \If{$previousWord \in NegationList$}
            \State $polarity \gets polarity * (-1)$
        \Else
            \If{$previousWord \in IntensityLexicon$}
                \State $polarity \gets polarity * intensity$
            \EndIf
        \EndIf
    \EndFor
    \State $score \gets polarity * sentiment$
\Else
    \If{$word \in EmojiLexicon$}
        \State $score \gets emojiSentiment$
    \EndIf
\EndIf 
\EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}




\section{Machine Learning Method}
\subsection{Naive Bayes}

Naive Bayes is a probabilistic classification model. It is based on Bayes theorem:
\begin{equation}
\label{eq:bayes}
            P(y|x) = \frac{P(y) * P(x|y)}{P(x)}
\end{equation}
where $y$ is the class label and $x$ is the a of attribute values \cite{DBLP:books/aw/TanSKK2019}. 

Bayes theorem allows to calculate the posterior probability $P(y|x)$, which Tan et al. describe as ''the probability of observing a class label $y$ for a data instance given its set of attribute values $x$.'' \cite[p.~418]{DBLP:books/aw/TanSKK2019}. 

To calculate the posterior probability, the class conditional probability $P(x|y)$ is needed, which describes the probability of observing a set of attribute values given a class. One approach to calculate the class conditional probability outlined by Tan et al. is to ''consider the fraction of training instances of a given class for every possible combination of attribute values'' \cite[p.~419]{DBLP:books/aw/TanSKK2019}. With a large number of attributes and values, this method becomes computationally infeasible due to the exponential growth of combinations. \cite{DBLP:books/aw/TanSKK2019}

Due to this, the Naive Bayes assumption is employed to estimate the class conditional probability. Naive Bayes uses conditional independence, which states that attribute values are only dependent on the class label and not each other. Thus, the class conditional probability can be calculated by the following formula:
\begin{equation}
\label{eq:naive_assumption}
            P(x|y) = \prod_{i=1}^{d}P(x_{i}|y)
\end{equation}
with $x$ containing $d$ attributes $\{x\textsubscript{1},x\textsubscript{2},...,x\textsubscript{i}\}$ \cite{DBLP:books/aw/TanSKK2019}.

Furthermore, $P(x)$ remains constant for every class label $y$, so the class that maximizes the following is chosen \cite{DBLP:books/aw/TanSKK2019}:
\begin{equation}
\label{eq:naive_final}
            P(y|x)\propto P(y)\prod_{i=1}^{d}P(x_{i}|y) 
\end{equation}      










