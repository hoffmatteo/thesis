\chapter{Related Work}
\label{cha:Chapter2_RelatedWork}
\iffalse

Length: 1-2 pages

Effort: ~2 weeks

2-3 Arbeiten maximal, die genauer betrachtet werden
Ruhig mehr Zitate --> aber nicht detailliert betrachten
Introduction to Data Mining --> zu generell, nur als Zitat
Hier nur im engsten Sinne


Content
\begin{itemize}
\item Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification Using Distant Supeusedrvision.
Technical Report. Standford.
\item Taboada or Serendio or Vader?
\item Khuc et al.
\end{itemize}

\fi

\TODO{Verweise}


\section{Machine Learning Classifier}
\label{sub:related_ml}
Initial research for Twitter sentiment analysis employed a machine learning classifier, which was previously shown to be effective in sentiment analysis for movie reviews \cite{GoBHaHua2009}. Go et al., for example, created their own data set of 1.6 million by searching for tweets containing specific emoticons. Based on the emoticon searched, they labeled the tweet as positive or negative. Additionally, they then processed tweets by removing usernames, links, and repeated letters. They evaluated three different classifiers: Multinomial Naive Bayes, Maximum Entropy, which is equal to Logistic Regression in a binary classification, and Support Vector Machine \cite{GoBHaHua2009}. These classifiers are explained in more detail in section \ref{sub:classifiers}.

They implemented these classifiers with several different feature models, including unigram (one word), bigram (two words), unigram and bigram (one or two words), and unigram + parts of speech, which takes the word type into account. They also applied the number of occurrences of a word as a feature for Naive Bayes and Maximum Entropy, while using the presence of a word for Support Vector Machine. The different possible feature models and further explanations are discussed in section \ref{sub:fund_mach}. The test data set consisted of 177 negative and 182 positive tweets, which were parsed from a variety of topics and manually labeled. They also compared the classifiers to a baseline, which consisted in counting the number of positive and negative words in a tweet and assigning the polarity with the higher count. The list was created by Twittratr and contained 174 positive and 185 negative words \cite{GoBHaHua2009}. 

They observed that with unigrams, Support Vector Machine achieved the highest accuracy at 82.2\%, with Multinomial Naive Bayes coming in second place at 81.3\%, compared to Maximum Entropy's 80.5\%. All classifiers outperformed the baseline, which had an accuracy of 65.2\%. When bigrams were evaluated, the accuracy slightly decreased for Maximum Entropy and Support Vector Machine, while it slightly increased for Naive Bayes. The highest overall accuracy was achieved by Maximum Entropy with unigrams and bigrams, at 83.0\%, while parts of speech decreased the accuracy for all classifiers compared to unigrams \cite{GoBHaHua2009}. 

\section{Lexicon-Based Method}
\label{sub:related_lexicon}

Due to the need for labeled data and the issue of domain dependence, approaches adopting more direct indicators, called lexicon-based methods, were evaluated. Thelwall et al. constructed an improved version of their SentiStrength algorithm and evaluated it on data sets from multiple different sources, such as Twitter. Their goal was to detect sentiment strength in short and informal English text. Thus, they collected positive and negative sentiment strength, and did not declare a tweet to be positive or negative. Both strengths were denoted on a scale of 1 to 5, with 1 signifying no sentiment and 5 denoting strong sentiment of each type. The algorithm used a list of 2,489 sentiment words that were annotated with their human-coded polarity and strength. It utilized a spelling correction algorithm, a booster word list to strengthen or weaken sentiment, an idiom list for common idioms, a negation word list to adapt the sentiment and an emoticon list. Furthermore, repeated letters and repeated punctuation increased the strength of the connected word. Finally, exclamation marks led to a minimum positive strength of 2, unless the sentiment was negative \cite{10.1002/asi.21662}.

The test data sets were taken from BBC Forum, Digg.com, MySpace, Runners World forum, Twitter, and YouTube comments. In total, the data sets contained 11,790 texts, with Twitter accounting for 4,218. The texts were coded by 1-3 independent evaluators using a common code book, with Twitter being rated by a single evaluator. They evaluated their SentiStrength 2 algorithm on each data set as well as all data sets combined. Additionally, they compared it with a supervised version and multiple machine learning classifiers, such as Support Vector Machine, Logistic Regression, and Naive Bayes, which are described in section \ref{sub:classifiers}. They had trouble training some of the classifiers on the combined data set due to its size and limited resources, which led to them using only Support Vector Machine for the combined data set. For Twitter, they achieved an accuracy of 59.2\% for positive strength and 66.1\% for negative strength. When allowing for a difference of +/- 1, the accuracy reached 94.2\% and 93.4\%, respectively. Most machine learning classifiers performed slightly better, especially on the Twitter and BBC data sets \cite{10.1002/asi.21662}.

\section{Hybrid Method}
While a lexicon-based method can only classify a tweet if it finds sentiment words in its lexicon, a hybrid approach can still classify tweets using other features. To achieve this, Khuc et al. built a distributed system for Twitter sentiment analysis. They divided their system into two components, a lexicon builder and a sentiment classifier. Their lexicon builder automatically constructed a sentiment polarity lexicon specifically for Twitter, employing emoticons such as ":)" as seed words and only considering bigram terms. In their experiments, this resulted in a lexicon of 2,411 positive and 1,018 negative phrases. The lexicon builder used a total of 384,397 tweets, of which 232,442 contained positive smileys and 151,955 contained negative smileys \cite{khuc}.

For sentiment classification, they compared the lexicon-based and hybrid approach and considered the positive, negative, and neutral class. The lexicon-based approach divided every tweet into sentences, with the score of a sentence determined by the sum of all sentiment words. The score of a tweet was then calculated as the total score of all sentences in it. They differentiated between (1) "WH-questions", such as "when" and "why" being followed by "do" or "did" and ending with one question mark, (2) exclamatory sentences, which end with exclamation marks, and (3) all other sentences. The "WH-question" sentences were always classified as neutral, while the score of an exclamatory sentence was multiplied to assign a higher weight. Finally, the other sentences were scored using the sum of their sentiment words detected by the built lexicon. They used a parts-of-speech tagger to only consider certain word types and also inverted the sign of a sentiment word if a negation was detected \cite{khuc}.

The hybrid method combined the aforementioned lexicon-based method with a Logistic Regression classifier. The approach behind Logistic Regression is illustrated in section \ref{sub:logistic}. They utilized the positive or negative score of the lexicon-based method as an additional feature for the machine learning classifier. If the lexicon-based method resulted in a neutral score, the feature was considered absent. Thus, even when the lexicon-based method was unable to score the tweet due to lack of coverage in its sentiment lexicon, the machine learning classifier was still able to classify using the other features. They trained the Adaptive Logistic Regression classifier with 70,000 tweets, of which 60,000 were taken from the lexicon builder data set, and 10,000 newly captured neutral tweets. This resulted in 36,282 positive tweets and 23,718 negative tweets. Their test data set was manually annotated and consisted of 584 positive, 270 negative, and 146 neutral tweets. With the lexicon-based method, they achieved an accuracy of 72.1\%, upon which the hybrid method improved to reach 73.7\% \cite{khuc}.










