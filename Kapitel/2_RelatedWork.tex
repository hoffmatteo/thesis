\chapter{Related work}
\label{cha:Chapter2_RelatedWork}
\iffalse

Length: 1-2 pages

Effort: ~2 weeks

2-3 Arbeiten maximal, die genauer betrachtet werden
Ruhig mehr Zitate --> aber nicht detailliert betrachten
Introduction to Data Mining --> zu generell, nur als Zitat
Hier nur im engsten Sinne


Content
\begin{itemize}
\item Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification Using Distant Supervision.
Technical Report. Standford.
\item Taboada or Serendio or Vader?
\item Khuc et al.
\end{itemize}

\fi
Initial research for Twitter sentiment analysis used a machine learning classifier, which was previously shown to be effective in sentiment analysis for Movie Reviews \cite{GoBHaHua2009}. Go et al., for example, created their own data set of 1.6 million tweets by searching for tweets containing specific emoticons. Based on the emoticon searched, they labeled the tweet as positive or negative. Additionally, they then processed tweets by removing usernames, links, and repeated letters. They evaluated three different classifiers: Multinomial Naive Bayes, Maximum Entropy, which is equal to using Logistic Regression in a binary classification, and Support Vector Machine using a linear kernel. For Naive Bayes and Maximum Entropy they used the number of occurrences as a feature, while they used presence for Support Vector Machine \cite{GoBHaHua2009}. 

They implemented these classifiers using several different feature models, including unigram, bigram, unigram + bigram, and unigram + parts of speech. The test data set consisted of 177 negative and 182 positive tweets, which were parsed from a variety of topics and manually labeled. They also compared the classifiers to a baseline, which consisted of counting the number of positive and negative words in a tweet and assigning the polarity with the higher count. The list was created by Twittratr and contained 174 positive and 185 negative words \cite{GoBHaHua2009}. 

They observed that with unigrams, Support Vector Machine achieved the highest accuracy at 82.2\%, with Multinomial Naive Bayes coming in second place at 81.3\%, compared to Maximum Entropy's 80.5\%. All classifiers outperformed the baseline, which had an accuracy of 65.2\%. When bigrams were evaluated, the accuracy slightly decreased for Maximum Entropy and Support Vector Machine, while it slightly increased for Naive Bayes. The highest overall accuracy was achieved by Maximum Entropy when using unigrams and bigrams, at 83.0\%, while Parts of Speech decreased the accuracy for all classifiers compared to unigrams \cite{GoBHaHua2009}. 

Due to the need for labeled data and the issue of domain dependence, approaches using more direct indicators, called lexicon-based methods, were evaluated. Thelwall et al. constructed an improved version of their SentiStrength algorithm and evaluated it on data sets from multiple different sources, among others, Twitter. Their goal was to detect sentiment strength in a short and informal English text. Thus, they collected positive and negative sentiment strength, and did not declare a tweet to be positive or negative. Both strengths were denoted on a scale of 1 to 5, with 1 signifying no sentiment and 5 signifying strong sentiment of each type. The algorithm used a list of 2489 sentiment words that were annotated with the human-coded polarity and strength. It utilized a spelling correction algorithm, a booster word list to strengthen or weaken sentiment, an idiom list for common idioms, a negation word list to adapt the sentiment and an emoticon list. Furthermore, repeated letters and repeated punctuation increased the strength of the connected word. Finally, exclamation marks led to a minimum positive strength of 2, unless the sentiment was negative \cite{10.1002/asi.21662}.

The test data sets were taken from BBC Forum posts, Digg.com posts, MySpace, Runners World forum posts, Twitter posts, and YouTube comments. In total, the data sets contained 11,790 texts, with Twitter accounting for 4,218. The texts were coded by 1-3 independent evaluators using a common code book, with Twitter being rated by a single evaluator. They evaluated their SentiStrength 2 algorithm on each data set as well as all data sets combined. Additionally, they compared it with a supervised version as well as multiple machine learning classifiers, such as Support Vector Machine, Logistic Regression, and Naive Bayes. They had trouble training some of the classifiers on the combined data set due to its size and limited resources, which led to them using only Support Vector Machine for the combined data set. For Twitter, they achieved an accuracy of 59.2\% for positive strength and 66.1\% for negative strength. When allowing for a difference of +/- 1, the accuracy reached 94.2\% and 93.4\%, respectively. Most machine learning classifiers performed slightly better, especially on the Twitter and BBC data sets \cite{10.1002/asi.21662}.

Because the lexicon-based method can only classify a tweet if it finds sentiment words in its lexicon, a hybrid approach can be used to still classify tweets using other features. For this, Khuc et al. built a distributed system for Twitter sentiment analysis using the MapReduce framework. They divided their system into two components, a lexicon builder and a sentiment classifier. Their lexicon builder automatically constructed a sentiment polarity lexicon specifically for the Twitter domain, using emoticons such as ":)" as seed words and building the co-occurence matrix between bigram phrases. In their experiments, this resulted in a lexicon of 2,411 positive phrases and 1,018 negative phrases. The lexicon builder used a total of 384,397 tweets, of which 232,442 contained positive smileys and 151,955 contained negative smileys \cite{khuc}.

For sentiment classification, they compared the lexicon-based and hybrid approach and considered the positive, negative, and neutral class. The lexicon-based approach divided every tweet into sentences, with the score of a sentence being the sum of all sentiment words in it. The score of a tweet was then calculated as the total score of all sentences in it. They differentiated between (1) "WH-questions", such as "when" and "why" being followed by "do" or "did" and ending with one question mark, (2) exclamatory sentences, which end with a exclamation mark, and (3) all other sentences. The "WH-question" sentences were always classified as neutral, while the score of an exclamatory sentence was multiplied to assign a higher weight. Finally, the other sentences were scored using the sum of their sentiment words detected by the built lexicon. They used a parts-of-speech tagger to only consider nouns, adjectives, adverbs, verbs, interjections, emoticons, abbreviations, and hashtags, and also inverted the sign of a sentiment word if a negation was detected \cite{khuc}.

The hybrid method combined the aforementioned lexicon-based method with the Adaptive Logistic Regression classifier. They used the binary score of the lexicon-based method, positive or negative, as an additional feature for the machine learning classifier. If the lexicon-based method resulted in a neutral score, the feature was considered absent. Thus, even when the lexicon-based method was unable to classify the tweet due to lack of coverage in its sentiment lexicon, the machine learning classifier could still classify using the other features. They trained the Adaptive Logistic Regression classifier using 70,000 tweets, of which 60,000 were taken from the data set which was used to train the lexicon builder, and 10,000 newly captured neutral tweets. This resulted in 36,282 positive tweets and 23,718 negative tweets. Their test data set was manually annotated and consisted of 584 positive, 270 negative, and 146 neutral tweets. With the lexicon-based method, they achieved an accuracy of 72.1\%, while the hybrid method achieved 73.7\% \cite{khuc}.










