\chapter{Related work}
\label{cha:Chapter2_RelatedWork}
\iffalse

Length: 1-2 pages

Effort: ~2 weeks

2-3 Arbeiten maximal, die genauer betrachtet werden
Ruhig mehr Zitate --> aber nicht detailliert betrachten
Introduction to Data Mining --> zu generell, nur als Zitat
Hier nur im engsten Sinne


Content
\begin{itemize}
\item Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification Using Distant Supervision.
Technical Report. Standford.
\item Taboada or Serendio or Vader?
\item Khuc et al.
\end{itemize}

\fi

Although a large number of Sentiment Analysis focuses on domains such as movie or product reviews, the work considered here concentrates on Twitter Sentiment Analysis. The current state of research is often divided into three different major classes: Machine Learning, Lexicon-Based, and Hybrid. \cite{DBLP:journals/csur/GiachanouC16}. Initial research utilized a machine-learning classifier, which was previously shown to be effective in Sentiment Analysis for Movie Reviews \cite{GoBHaHua2009}. Go et al., for example, created their own data set of 1.6 Million tweets, by searching for tweets containing an emoticon. Based on the emoticon searched, they labeled the tweet as positive or negative. Additionally, they then processed tweets by removing usernames, links, and repeated letters. They evaluated three different classifiers: multinomial Naive Bayes, Maximum Entropy, which equal to using Logistc Regression in a binary classification, and Support Vector Machines using a linear kernel. For Naive Bayes and Maximum Entropy they use the number of occurrences of a feature, while they use feature presence for Support Vector Machines \cite{GoBHaHua2009}. 

They implemented these classifiers using several different feature models, including Unigram, Bigram, Unigram + Bigram, and Unigram + Parts of speech. The test data set consisted of 177 negative and 182 positive tweets, which were manually labeled, parsed from a variety of topics. They also compared the classifiers to a baseline, which consisted of counting the number of positive and negative words in a tweet, assigning the polarity with the higher count. The list was created by Twittratr and contained 174 positive and 185 negative words \cite{GoBHaHua2009}. 

They concluded that with Unigrams, Support Vector Machines achieved the highest accuracy of 82.2\%, with Naive Bayes coming in second place at 81.3\%, compared to Maximum Entropy's 80.5\%. All classifiers outperformed the baseline, which had an accuracy of 65.2\%. When Bigrams were evaluated, the accuracy slightly decreased for Maximum Entropy and Support Vector Machines, while it slightly increased for Naive Bayes. The highest overall accuracy was achieved by Maximum Entropy when using Unigrams + Bigrams, at 83.0\%, while Parts of Speech decreased the accuracy for all classifiers \cite{GoBHaHua2009}. 

Due to the need for labeled data and the issue of domain dependence, approaches using more direct indicators, called lexicon-based methods, were implemented. Thelwall et al. constructed an improved version of their SentiStrength algorithm and evaluated it on data sets from multiple different sources, such as Twitter. Their goal was to detect sentiment strength in a short and informal English text. Thus, they collect positive and negative sentiment strength, and do not declare a tweet to be positive or negative. Both strengths are denoted on a scale of 1 to 5, with 1 signifying no sentiment and 5 signifying strong sentiment of each type. The algorithm uses a list of 2489 sentiment words that contain human-coded polarity and strength. It utilizes a spelling correction algorithm, a booster word list to strengthen or weaken sentiment, an idiom list for common idioms, a negation word list to adapt the sentiment and an emoticon list. Furthermore, repeated letters and repeated punctuation boost the strength of the connected word. Finally, exclamation marks lead to a minimum positive strength of 2, unless the sentiment is negative \cite{10.1002/asi.21662}.

The test data sets were taken from BBC Forum posts, Digg.com posts, MySpace, Runners World forum posts, Twitter posts, and YouTube comments. In total, the data sets contained 11,790 texts, with Twitter accounting for 4,218. The texts were coded by 1-3 independent evaluators using a common code book, with Twitter being rated by a single evaluator. They evaluated their SentiStrength 2 algorithm on each data set as well as all data sets combined. Additionally, they compared it to a supervised version, as well as multiple machine-learning classifiers, for example, Support Vector Machines, Logistic Regression and Naive Bayes. They had trouble training some of the classifiers on the combined data set due to its size and limited resources, so they only used Support Vector Machines for this data set. For Twitter, they achieved an accuracy of 59.2\% for positive strength and 66.1\% for negative strength. When allowing for a difference of +/- 1, the accuracy reached 94.2\% and 93.4\%, respectively. Most machine-learning classifiers performed slightly better, especially on the Twitter and BBC data sets \cite{10.1002/asi.21662}.

Because the lexicon-based method can only classify a tweet if it finds sentiment words in its lexicon, a hybrid approach can be used to still classify tweets using other features. For this, Khuc et al. built a distributed system for Twitter Sentiment Analysis using the MapReduce framework. They divide their system into two components, a lexicon builder and a sentiment classifier. Their lexicon builder automatically constructs a sentiment polarity lexicon specifically for the Twitter domain, using emoticons such as ":)" as seed words and building the co-occurence matrix between bi-gram phrases. In their experiments, this resulted in a lexicon of 2,411 positive phrases and 1018 negative phrases, from a total of 384,397 tweets, of which 232,442 contained positive smileys and 151,955 contained negative smileys. For the sentiment classification, they compared two approaches and considered the positive, negative, and neutral class \cite{khuc}.

The lexicon-based approach divided every tweet into its sentences, with the score of a sentence being the sum of sentiment words/phrases in it. The score of a tweet is then calculated as the total score of all sentences in it. They differentiated between WH-questions, such as "when" and "why" being followed by "do" or "did" and ending with one question mark, exclamatory sentences, which end with a exclamation mark, and all other sentences. The "WH-question" sentences are always classified as neutral, while the score of an exclamatory sentence is multiplied to assign a higher weight. Finally, the other sentences are scored based on the sum of their sentiment words/phrases using the built lexicon. They used a POS tagger to only consider nouns, adjectives, adverbs, verbs, interjections, emoticons, abbreviations, and hashtags, and also inverted the sign of a sentiment word if a negation was detected \cite{khuc}.

The hybrid method combined the aforementioned lexicon-based method with the Adaptive Logistic Regression classifier. They used the binary score of the lexicon-based method, positive or negative, as an additional feature for the machine-learning classifier. If the lexicon-method resulted in a neutral score, the feature was considered absent. Thus, even when the lexicon-based method is unable to classify the tweet because of a lack of coverage in its sentiment lexicon, the machine-learning classifier can still classify using the other features. They trained the Adaptive Logistic Regression classifier using 70,000 tweets, of which 60,000 were taken from the data set which was used to train the lexicon builder, and 10,000 newly captured neutral tweets. This resulted in 36,282 positive and 23,718 negative tweets. Their test data set was manually annotated and consisted of 584 positive, 270 negative, and 146 neutral tweets. With the lexicon-based method, they achieved an accuracy of 72.1\%, while the hybrid method achieved 73.7\% \cite{khuc}.










