\section{Evaluation}

\subsection{Test data}

The test data contained human-annotated tweets provided by two different sources and covered a wide variety of topics. In total, 10,372 tweets were used, of which 1,776 were negative, 4,968 neutral, and 3,628 positive.

\subsubsection{SentiStrength}
SentiStrength is a data set constructed by Thelwall et al. to evaluate their SentiStrength algorithm, a lexicon-based classifier. They created data sets from multiple different sources including Twitter, BBC Forum, and other social media platforms. In this thesis, only the Twitter data set is used, which was labeled by one person \cite{10.1002/asi.21662}.

The data set contains three columns, an integer score of 1 to 5 for the positive score, a separate integer score of 1 to 5 for the negative score, and the tweet text. If a score is 1, it signifies that there is no sentiment, while a 5 signals a strong sentiment. \cite{10.1002/asi.21662}. Because a tweet can have both a considerable positive and negative score, an approach inspired by Saif et al. is used to convert it to a single polarity. A tweet is defined as positive if the positive score is at least 1.5 times higher than the negative one, and vice versa. If neither score is at least 1.5 greater, the tweet is classified as neutral \cite{oro40660}. This resulted in 947 negative, 1,959 neutral and 1,336 positive tweets.

\TODO{abgewandelte Version davon, wie zitieren?, topics?}

\subsubsection{SemEval2013}
According to Giachanou and Crestani, "SemEval (Semantic Evaluation) is an ongoing series of evaluations of computation semantic analysis systems" \cite[p.~28:31]{DBLP:journals/csur/GiachanouC16}. In 2013, they constructed multiple data sets for both training and testing. They defined two subtasks, subtask A to determine the sentiment based on the context of a marked word/phrase, and subtask B to detect the sentiment based on the entire message. Because this thesis looks at tweets as a whole without context, only the second data set is used \cite{nakov-etal-2013-semeval}.

The tweets were parsed based on popular topics that were identified earlier, and tweets that did not contain sentiment-bearing words were filtered out. Finally, the messages were annotated by five Amazon Mechanical Turk workers, for subtask B the polarity selected by the majority was chosen \cite{nakov-etal-2013-semeval}. As the data set only provided the tweet IDs and not the text, the Twitter API was used to crawl the text for each tweet. This was not possible for every tweet contained in the data set, as some were already deleted or otherwise not available. This resulted in 6,130 tweets, of which 829 were negative, 3,009 neutral and 2,292 positive.

\TODO{task}

\begin{table}[h!]
\centering
\caption{Table to test captions and labels.}
\begin{tabular}{ |p{3cm}||p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
 \hline
 Classifier name &          Parameters &             Training Instances &    Accuracy &      Kappa &     Relative absolute error \\
 \hline
 Naive Bayes (Gauss)        &-&            TODO&                 60.99\%&        0.21&       82.14\%\\
  \hline
 Naive Bayes (Multinomial)  &-&                     1231862&                78.67\%&        0.51&       52.22\%\\
  \hline
 Random Forest              &depth = 300&            12318&                 75.48\%&        0.45&       81.33\%\\
  \hline
 Logistic Regression        &iterations = 10&            94655&                 77.40\%&        0.47&       59.70\%\\
  \hline
 Support Vector Machine     &kernel = linear&            TODO&                 77.11\%&        0.48&       45.52\%\\
 \hline
\end{tabular}
\label{tab:evaluations}
\end{table}