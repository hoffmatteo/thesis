\chapter{Evaluation}
\label{cha:Chapter6_Evaluation}
\section{Lexicon-Based Method}

The binary classification performance by the lexicon-based method can be seen in Table \ref{tab:eval_lex}. Overall, the lexicon-based method achieved good results, with an accuracy of 71.04\%. The 6 percentage point difference between recall and precision suggests that the classifier is very accurate when predicting a positive sentiment, but does not make that prediction often enough. Out of the 5,404 test instances, 544 were classified as neutral. In 509 of them, no sentiment word was detected, while in another 35 instances, the sentiment words balanced each other out. Taking into account the confusion matrix seen in Table \ref{tab:evaluations_conf_lex}, the method appears to be more accurate when scoring positive tweets, while the neutral instances are evenly distributed. Another interesting observation is that, for positive tweets, the number of false negatives and false neutrals is very close, especially compared to negative tweets. This may suggest that an improvement for positive instances should focus on correctly predicting the false neutrals, instead of the false negatives, as this may be an easier task. For example, identifying certain idioms may be easier to implement than, for example, correctly detecting sarcasm.

Another interesting observation can be made when the neutral test instances are included. The accuracy dropped to 52.56\%. The reason for this sharp drop is the difficulty of trying to predict neutral instances with an approach that mainly focuses on polarity classification. Of 4,968 neutral test instances, about half of the total instances, only 1,613 were correctly predicted. Of these correct neutrals, the large majority were classified as neutral because no sentiment words could be detected. Once a sentiment word was detected in a neutral instance, it was more likely to be classified as positive or negative than neutral. When considering Algorithm \ref{lexiconAlg}, this makes sense, as it is very rare for sentiment words to add up to 0. Even when rounding was introduced to make the margin for neutral instances wider, such as between -0.5 and +0.5, the accuracy did not increase significantly. This is due to the fact that although more neutral instances were correctly scored, more positive and negative instances were incorrectly classified as neutral due to rounding.

These results show that when the described lexicon-based method is applied to sentiment polarity classification, a high accuracy can be achieved. If neutrality is also to be considered, the performance drops significantly. This suggests that while a lexicon-based method is capable of detecting neutrality/objectivity, a different approach should be used for this task, such as different types of lexicons.

\begin{table}
\caption{Classification performance of the lexicon-based method.}
\label{tab:eval_lex}
\centering
\begin{tabular}{ |p{3cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
 \hline
 Method &          Accuracy &      Recall &     Precision& F-score \\
  \hline
 Lexicon-based & 71.04\% & 77.51\% & 83.77\% & 80.52\% \\
 \hline
 \end{tabular}
 
\end{table}
\begin{table}
\centering
\caption{Confusion matrix of the lexicon-based method.}
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}| }
 \hline
  &          Predicted as $positive$ &Predicted as $negative$  & Predicted as $neutral$ (0.0)\\
 \hline
 Are $positive$        & 2812&            476 & 340\\
  \hline
 Are $negative$  &545&                     1027 & 204\\
 \hline

\end{tabular}
\label{tab:evaluations_conf_lex}
\end{table}


\section{Machine Learning Method}

In Table \ref{tab:evaluations_max}, each classifier can be seen with its optimal parameters, the maximum number of training instances used, and the evaluation parameters discussed. In Table \ref{tab:evaluations_euqal}, each classifier was trained with the same number of instances for better comparison. The highest accuracy was achieved by Multinomial Naive Bayes, with an accuracy of 79.42\% and an F-score of 84.88\%. The confusion matrix for Multinomial Naive Bayes can be seen in Table \ref{tab:evaluations_conf}, which shows that the classifier seems to be able to handle positive tweets better than negative ones.

\begin{table}
\centering
\caption{Classification performance of each classifier when using the maximum number of training instances.}
\begin{tabular}{ |p{3cm}||p{3cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}| }
 \hline
 Classifier name &          Parameters &Training Instances &    Accuracy &      Recall &     Precision& F-score \\
 \hline
 Gaussian Naive Bayes       &-&            1,600,000&   66.03\%&        67.90\%&       78.59\%& 72.85\%\\
  \hline
 Multinomial Naive Bayes  &-&             1,600,000&      79.42\%&        86.03\%&       83.76\%& 84.88\%\\
  \hline
 Random Forest              &depth = 300&            120,000& 77.76\%&  82.91\%&    83.79\%& 83.35\%\\
  \hline
 Logistic Regression        &iterations = 10&   95,000&    76.94\%&        83.27\%&   82.54\%& 82.90\%\\
  \hline
 Support Vector Machine     &kernel = linear&            150,000&   77.78\%&        84.43\%&    82.81\%& 83.61\%\\
 \hline
\end{tabular}
\label{tab:evaluations_max}
\end{table}


\begin{table}
\centering
\caption{Classification performance of each classifier when using the same number of training instances.}
\begin{tabular}{ |p{3cm}||p{3cm}|p{2cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}|p{1.5cm}| }
 \hline
 Classifier name &       Parameters &    Training Instances &Accuracy &  Recall &     Precision& F-score \\
 \hline
 Gaussian Naive Bayes &-&     95,000&                 65.30\%& 67.97\%&       77.57\%& 72.45\%\\
  \hline
 Multinomial Naive Bayes  &-&     95,000& 77.31\%&        82.61\%&       83.44\%& 83.02\%\\
  \hline
 Random Forest              &depth = 300&            95,000&76.81\%&  81.89\%&     83.29\%& 82.59\%\\
  \hline
 Logistic Regression        &iterations = 10&   95,000&    76.94\%&        83.27\%&   82.54\%& 82.90\%\\
  \hline
 Support Vector Machine     &kernel = linear&   95,000&     76.70\%& 82.97\%&  82.44\%& 82.70\%\\
 \hline
\end{tabular}
\label{tab:evaluations_euqal}
\end{table}
\begin{table}
\centering
\caption{Confusion matrix for the Multinomial Naive Bayes classifier.}
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}| }
 \hline
  &          Predicted as $positive$ &Predicted as $negative$  \\
 \hline
 Are $positive$        & 3121&            507\\
  \hline
 Are $negative$  &605&                     1171\\
 \hline

\end{tabular}
\label{tab:evaluations_conf}
\end{table}
The topic of word frequency versus binary presence can be analyzed. Looking back at Equation \eqref{eq:Multinomial_bayes}, the posterior probability $P(d_i|c_j)$ was calculated using the probability that a word occurs in the given class, $P(w_t|c_j)$ to the factor of $N_{it}$, the number of times $w_t$ appears in a document $d_i$. Here, it is  assumed that the frequency of a word that appears in a tweet is utilized. This is a logical assumption to make, as the number of times a word occurs in a tweet can impact its sentiment, such as in the sentence "This is bad, really bad". In general, more information for the classifier to work with seems to be a good approach. However, when applied to Multinomial Naive Bayes, the accuracy slightly decreased to 78.00\% when using word frequency compared to word presence. Schneider also observed this behavior and determined the multinomial distribution as the cause \cite{nb_freq}. As proven by Eyheramendy et al., the Multinomial Naive Bayes model is a version of the Naive Bayes Poisson model, which assumes independence of document length and class \cite{Poisson}. Furthermore, Schneider cites \cite{Poisson_words} and states that terms are more likely to appear a second time than at all. He concluded that this does not fit the Poisson distribution well, which is why the reduction to a binary presence of words improves accuracy \cite{nb_freq}.

Additionally, the usefulness of n-grams can be discussed. The performance of each feature model can be seen in Table \ref{tab:eval_features}. Here, the combination of unigrams and bigrams was shown to be optimal, achieving the highest accuracy and F-Score. Employing only unigrams resulted in a slightly lower performance, while bigrams produced a much lower accuracy. This behavior could be explained by unigrams offering the most coverage, as only the presence of a single word in a tweet is needed to use it as a feature, while bigrams need a specific word combination to occur. As additional information, bigrams offer the ability to take certain sentence structures into account, which is why the accuracy of the combination is higher compared to only unigrams.

\begin{table}
\centering
\caption{Classification performance of Multinomial Naive Bayes with different features.}
\begin{tabular}{ |p{4cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}| }
 \hline
 Feature &     Accuracy &      Recall &     Precision& F-score \\
 \hline
 Unigram & 78.37\% & 84.85\% & 83.29\% & 84.03\% \\
  \hline
 Bigram & 70.53\% & 70.53\% & 82.98\% & 76.25\% \\
 \hline
 Unigram + Bigram  & 79.42\%&        86.03\%&       83.76\%& 84.88\%\\
 \hline
\end{tabular}
\label{tab:eval_features}
\end{table}


Another interesting observation is the distribution used for Naive Bayes. In section \ref{sub:nb}, both the multinomial and the Gaussian distribution were applied. The multinomial implementation clearly outperformed the Gaussian one. From this, it can be gathered that the multinomial distribution seems to better reflect the actual distribution of word occurrences.

Finally, performance should be noted. Only the two Naive Bayes implementations were able to handle all 1.6 million training instances. Additionally, the Multinomial Naive Bayes had a much shorter runtime. For the other three classifiers, using too many training instances would result in a memory error or a runtime of more than 12 hours. Even after assigning more memory to the program, the issues still persisted at a certain number of instances. Due to this, the Multinomial Naive Bayes seems to offer the best mixture of performance and accuracy.

To summarize, machine-learning classifiers offer very high accuracy, with Multinomial Naive Bayes achieving almost 80\%. However, this accuracy depends on both the quality and the size of the training data \cite{DBLP:journals/csur/GiachanouC16}. Thus, the disadvantages are also clear. The need for labeled training data, as well as the long and unpredictable runtime of some classifiers, resulted in this method being much more time-consuming and complex than the lexicon-based method. 

\section{Hybrid Method}

The performance of each method can be seen in Table \ref{tab:evaluations_hybrid}. Both hybrid methods are an improvement on either of the other methods, with the first approach achieving the highest overall accuracy at 80.92\%, while the second approach resulted in the highest F-score at 86.09\%. Despite the slighty lower F-score, the first approach appears to be more balanced overall, as recall, precision, and F-score are roughly equal at 85-86\%. This suggests that the lexicon-based score as a feature is able to provide some stability to the machine learning method. Using the lexicon-based method as a labeling mechanism also proved to be more effective than simply assigning labels based on an emoticon. This could be explained by the labels being more accurate, but also by the reduction to less, but more opinionated training instances, as only tweets with sentiment words were used.

Although the accuracy improved, a hybrid method also adds further complexity to the process. In this thesis, both the lexicon-based method and the machine learning classifier were already implemented, which reduced the additional effort. However, if no existing methods are to be used, developing and testing two different approaches may be too laborious compared to the improvement in accuracy over only using the machine learning approach.

\begin{table}
\caption{Classification performance of the two hybrid approaches.}
\centering
\begin{tabular}{ |p{5.5cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
 \hline
 Method &          Accuracy &      Recall &     Precision& F-score \\
 \hline
 Lexicon-based score as feature      & 80.92\%&        86.44\%&       85.33\%& 85.88\%\\
  \hline
 Lexicon-based score as label   & 80.79\%&        88.53\%&       83.78\%& 86.09\%\\
  \hline
\end{tabular}
\label{tab:evaluations_hybrid}
\end{table}

\iffalse
\begin{table}
\centering
\caption{Confusion matrix Method 1.}
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}| }
 \hline
  &          Predicted as $positive$ &Predicted as $negative$  \\
 \hline
 Are $positive$        & 3088&            540\\
  \hline
 Are $negative$  &532&                     1244\\
 \hline

\end{tabular}
\label{tab:method1_conf}
\end{table}

\fi



