\chapter{Methodology and Implementation}
\label{cha:Chapter4_Methodology}

Length: 20-25 pages

Effort: 8 weeks+

Teilung zwischen Methodik (eher abstrakt) und Implementierung
Damit beginnen --> also mit Implementierung

Questions:
\begin{itemize}
\item Structure --> differentiation between getting the data, analysing the data and evaluating the results ok? --> Kann man machen
\item General idea: ``Readers should be able to carry out the same procedure using the thesis'' --> level of detail, e.g. cloud services? --> Implementierung auch auf Github zur Verfuegung stellen, API auf hoeherem Level (was machen die Funktionen verwenden),
Falls Cloud: nur virtueller Rechner --> eher nicht wichtig, falls spezifische Dienstleistung z.B. OpenAI (KI, Parsing) --> dann detaillierter, eher Richtung Implementation
\item Case study --> Evaluation of accuracy based on one real life topic? Topic, e.g. Covid-19 pandemic, german elections (aufpasssen, Politik schwer!), ...? --> je nach Umfang auch mehrere (sollen sich unterscheiden, Covid sehr viele Bereiche), Tweets auch auf Englisch
\end{itemize}

Content
\begin{itemize}
\item Data 
\begin{itemize}
    \item What data does twitter provide, auch z.B. Likes, Retweets
    \item Data extraction, Cloud Services --> spezifischer Dienst?
\end{itemize}
\item Analysis
\begin{itemize}
    \item Lexicon-Based Method
    \item Machine Learning Based Method
    \item Hybrid Method
\end{itemize}
\item Evaluation
\begin{itemize}
    \item Parameters --> objektive Kriterien, Qualitaet der Analyse Methode + Begruendung der Kriterien, Komplexitaet (Laufzeitkomplexitaet ja, Implementierung --> an sich nicht schlimm, aber robust? Abstuerze?)
    \item Case Study
    \item Qualitaet der Ergebnisse --> gut/schlecht, kann das akkurat erkannt werden?, evtl. menschliche Analyse, schoener: auf subjektive Einschaetzung verzichten
    \item Comparison with human tweets labeled by humans?
\end{itemize}
\end{itemize}


Analysis
\begin{itemize}
    \item Lexicon-Based Method
    \begin{itemize}
    \item General approach: Multiple lexicons with different topics, positive/neutral/negative
    \begin{itemize}
        \item Sentiment Words --> List of words with polarity, e.g. good
        \item Negation Words --> Words that negate the polarity of other words, e.g. not
        \item Intensity words --> Words that amplify the polarity, e.g. very
        \item Emoticons
    \end{itemize}
    \item Find the above word types in tweet
    \item Adjust polarity of sentiment words based on negation and/or intensity
    \item Calculate average of polarities including emoticons
    \item Example: The weather today is very terrible but the sound of rain is delightful.
    \begin{itemize}
        \item Sentiment of "very terrible": amplified negative (e.g. -2)
        \item Sentiment of "delightful": positive (e.g. +1)
        \item Overall: -2 + 1 = -1 --> negative
    \end{itemize}
    \item Comparison of different sentiment lexicons?
    \item Stop words --> needs, wants, etc. --> automatically negative (except for negation?)
    \item common idioms?
    \item slang
    \end{itemize}
    \item Machine Learning Based Method
    \begin{itemize}
        \item Naive Bayes --> probabilistic, multiple classes
        \begin{itemize}
            \item Based on Bayes theorem with conditional independence, add-one (laplace) smoothing, calculate P(x|y)
            \item Look at every word contained in tweet, apply formula to known words:
\[P(\mathrm{w}_{i}^{}|c) = \frac{count(\mathrm{w}_{i}^{}, c)+1}{(\sum_{w\in V}^{} (count(w,c)) + \left| V \right|} \]
            \item w = word, c is class (negative, positive), V is vocabulary of all known words
            \item Train with labelled tweets
           \[ \mathrm{c}_{NB}^{} = argmax_{c \in C}  P(c) \prod_{i \in positions}^{} P(\mathrm{w}_{i}^{}|c)\]
          
           
        \end{itemize}
        \item Logistic Regression? --> discriminative, binary classes
        \begin{itemize}
        \item Advantage: Not as many assumptions --> works better even when some features are correlated
        \end{itemize} 
        
    \end{itemize}
    \item Hybrid Method
    \begin{itemize}
        \item Approach 1 --> use lexicon-based score as an additional feature for the classifier (even if 0, it still can be classified)
        \item Combine the previous two or e.g. use a different classifier if that works better?
    \end{itemize}
\end{itemize}
\

New Learnings
\begin{itemize}
    \item Evaluation of different classifiers --> (J48), Naive Bayes (gaussian + multinominal distribution), Support Vector Machine, Logistic Regression, Random Forest
    \item Currently best: Naive Bayes with a multinomial distribution, 78.67\% of 4726 instances correct
    \item Caveats: Training for SVM and Logistic Regression not possible with all 1.2MM tweets --> system memory/runtime
    \item TODO: currently the ML method only recognized positive and neutral, more test instances?
    \item Lexicon Method: 75.68\% accurate on 9057 instances (includes neutral)
\end{itemize}

RandomForest
\begin{itemize}
    \item Collection of tree-structured classifiers
    \item Each tree depends on values of a random, independent vector
    \item Output is class selected by most trees
\end{itemize}

Questions
\begin{itemize}
    \item Fundamentals vs. Methodology: Wo z.B. Classifiers erklären?
    \item Welche Classifier wie vertiefen?
    \begin{itemize}
    \item Tabelle die Classifier vergleicht
    \item Kurze Erklärung aller Classifier?
    \item "Besten" Classifier tiefer erklären?
    \item z.B. Naive Bayes: Multinominale und Gaußsche Verteilung vergleichen/erklären?
    \end{itemize}
    \item Spezifische Bibliothek erwähnen (weka), Filter genauer (speichert nur 15000 Wörter, die mindestens 10x vorkommen, lowercase)?
    \item Daten beschreiben: Referenz + kurzer Überblick (Anzahl positiv/neutral/negativ, Themen, Zeitraum, bei Testdaten die Methode zur Klassifikation)
\end{itemize}





    

        
    
        
        












